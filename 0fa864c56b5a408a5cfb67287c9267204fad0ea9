{
  "comments": [
    {
      "unresolved": true,
      "key": {
        "uuid": "46b16ab7_7106165e",
        "filename": "/COMMIT_MSG",
        "patchSetId": 7
      },
      "lineNbr": 20,
      "author": {
        "id": 1003883
      },
      "writtenOn": "2021-10-29T01:39:04Z",
      "side": 1,
      "message": "If we are going to be able to split up larger pushes into smaller pushes for scaling purposes, then I would like to ensure that we do it in a way that each of the smaller pushes could potentially be done by different primaries concurrently. Does the solution offer this possibility? If not, what approach would?",
      "revId": "0fa864c56b5a408a5cfb67287c9267204fad0ea9",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "d61923ac_97e29c9d",
        "filename": "/COMMIT_MSG",
        "patchSetId": 7
      },
      "lineNbr": 20,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2021-10-29T23:02:02Z",
      "side": 1,
      "message": "That is not the purpose as documented in Issue 13216. A target replication end like GitHub simply won\u0027t accept pushes with lots of refs: splitting a large push as a series of smaller and sequential pushes is the only way to replicate to those endpoints. It actually won\u0027t make it faster but slower than a single push with all the refs, but at least it will eventually succeed instead of always failing and retrying forever.\n\nPlease note that would \"solve\" a GitHub (or GitLab) problem by \"workaround\" on the Gerrit side.",
      "parentUuid": "46b16ab7_7106165e",
      "revId": "0fa864c56b5a408a5cfb67287c9267204fad0ea9",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "da8e5a55_960debe6",
        "filename": "/COMMIT_MSG",
        "patchSetId": 7
      },
      "lineNbr": 20,
      "author": {
        "id": 1025452
      },
      "writtenOn": "2021-10-30T08:03:59Z",
      "side": 1,
      "message": "But more importantly it is disabled by default and can be enabled/fine-tuned per remote.\n\n@Martin the idea is interesting but it would imply that division into batches happens on the scheduling level, gets serialised and individual batches could be picked up by any primary, right? Sounds like a lot work to me (but I could be wrong). As it was said by Luca - that is not the purpose of this change and AFAIU single master would group it back to single push :D",
      "parentUuid": "d61923ac_97e29c9d",
      "revId": "0fa864c56b5a408a5cfb67287c9267204fad0ea9",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "f4df6ea2_d1f92624",
        "filename": "/COMMIT_MSG",
        "patchSetId": 7
      },
      "lineNbr": 20,
      "author": {
        "id": 1003883
      },
      "writtenOn": "2021-10-31T23:38:41Z",
      "side": 1,
      "message": "\u003e That is not the purpose as documented in Issue 13216. A target replication end like GitHub simply won\u0027t accept pushes with lots of refs: splitting a large push as a series of smaller and sequential pushes is the only way to replicate to those endpoints. It actually won\u0027t make it faster but slower than a single push with all the refs, but at least it will eventually succeed instead of always failing and retrying forever\n\nMakes sense. Whatever approach you take, I think it is important that you ensure that the on disk data reflect whatever replication is doing, and what replication work is outstanding. This is important not only to let other primaries potentially help out, but also to ensure that benefit of splitting the refs into smaller batches not be lost by primary restarts. In another words, if you manage to replicate 9K refs out of 10K refs to a site (say in batches of 1K), and then the primary goes down, it is important that when the server starts up again, that it not try to re-replicate those 9K refs which were already successful. Does your current approach do this?",
      "parentUuid": "d61923ac_97e29c9d",
      "revId": "0fa864c56b5a408a5cfb67287c9267204fad0ea9",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "d95c16d8_2ecfecd9",
        "filename": "/COMMIT_MSG",
        "patchSetId": 7
      },
      "lineNbr": 20,
      "author": {
        "id": 1025452
      },
      "writtenOn": "2021-11-02T07:49:22Z",
      "side": 1,
      "message": "AFAIU it will not be lost as `generateUpdates` asks remote for existing refs. IOW it will not try to resend 9K refs but only the remaining 1K as previous pushes succeeded. IMHO this looks like a small improvement over what we have now cuz if I\u0027m not mistaken we would push all 10K refs now... Having said that I believe that importance of having it accounted on the disk is not that important anymore ;).",
      "parentUuid": "f4df6ea2_d1f92624",
      "revId": "0fa864c56b5a408a5cfb67287c9267204fad0ea9",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    }
  ]
}