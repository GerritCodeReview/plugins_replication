{
  "comments": [
    {
      "key": {
        "uuid": "d19be536_1dd69a48",
        "filename": "/COMMIT_MSG",
        "patchSetId": 12
      },
      "lineNbr": 11,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2020-08-11T22:29:14Z",
      "side": 1,
      "message": "Once Change-Id: I224c2ce is merged, the statement may not be true anymore.",
      "range": {
        "startLine": 10,
        "startChar": 26,
        "endLine": 11,
        "endChar": 27
      },
      "revId": "75430697bdcfa2daa1b41c57b673d13d9e797ed4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "d3249d2e_5fdfbbd9",
        "filename": "/COMMIT_MSG",
        "patchSetId": 12
      },
      "lineNbr": 11,
      "author": {
        "id": 1003883
      },
      "writtenOn": "2020-08-11T22:47:24Z",
      "side": 1,
      "message": "\u003e Once Change-Id: I224c2ce is merged, the statement may not be true anymore.\n\nI will try to  update this commit message if this no longer is the case.",
      "parentUuid": "d19be536_1dd69a48",
      "range": {
        "startLine": 10,
        "startChar": 26,
        "endLine": 11,
        "endChar": 27
      },
      "revId": "75430697bdcfa2daa1b41c57b673d13d9e797ed4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "adca82d5_41859ec7",
        "filename": "/COMMIT_MSG",
        "patchSetId": 12
      },
      "lineNbr": 24,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2020-08-11T22:29:14Z",
      "side": 1,
      "message": "When there is no load, they will still be serialised. Where is the mechanism that makes them faster in absence of load?",
      "range": {
        "startLine": 24,
        "startChar": 27,
        "endLine": 24,
        "endChar": 39
      },
      "revId": "75430697bdcfa2daa1b41c57b673d13d9e797ed4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "3d962158_a5bb6943",
        "filename": "/COMMIT_MSG",
        "patchSetId": 12
      },
      "lineNbr": 24,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2020-08-11T22:29:14Z",
      "side": 1,
      "message": "I did not find any throttling mechanism in the ChainedScheduler: could you provide a source file + line number of where that is implemented? What I see from the code is a serial firing of the events using a stream.",
      "range": {
        "startLine": 23,
        "startChar": 56,
        "endLine": 24,
        "endChar": 17
      },
      "revId": "75430697bdcfa2daa1b41c57b673d13d9e797ed4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "eef6cc70_f7157a9d",
        "filename": "/COMMIT_MSG",
        "patchSetId": 12
      },
      "lineNbr": 24,
      "author": {
        "id": 1003883
      },
      "writtenOn": "2020-08-11T22:47:24Z",
      "side": 1,
      "message": "\u003e I did not find any throttling mechanism in the ChainedScheduler: could you provide a source file + line number of where that is implemented? What I see from the code is a serial firing of the events using a stream.\n\nLine 104 here:\nhttps://gerrit-review.googlesource.com/c/plugins/replication/+/267812/12/src/main/java/com/googlesource/gerrit/plugins/replication/ChainedScheduler.java\nThe next event is scheduled before executing the current event. Since an event\u0027s Task must be running before it can schedule another one, only one event can ever be scheduled concurrently. However nothing prevents multiple event Tasks from running at the same time, and the next one is scheduled before executing the current event. If there are free threads then it is possible for more than one event to be executing at the same time. This is what I mean by \"natural\", no special code is required to throttle this way, the \"scheduleNext then execute\" ordering takes care of the throttling.",
      "parentUuid": "3d962158_a5bb6943",
      "range": {
        "startLine": 23,
        "startChar": 56,
        "endLine": 24,
        "endChar": 17
      },
      "revId": "75430697bdcfa2daa1b41c57b673d13d9e797ed4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "4102add1_3875b4b9",
        "filename": "/COMMIT_MSG",
        "patchSetId": 12
      },
      "lineNbr": 24,
      "author": {
        "id": 1003883
      },
      "writtenOn": "2020-08-11T22:47:24Z",
      "side": 1,
      "message": "\u003e When there is no load, they will still be serialised. Where is the mechanism that makes them faster in absence of load?\n\nThe sentence on lines 22-23 explains how this throttling (and parallelism) is achieved.",
      "parentUuid": "adca82d5_41859ec7",
      "range": {
        "startLine": 24,
        "startChar": 27,
        "endLine": 24,
        "endChar": 39
      },
      "revId": "75430697bdcfa2daa1b41c57b673d13d9e797ed4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "d5559254_17a36117",
        "filename": "/COMMIT_MSG",
        "patchSetId": 12
      },
      "lineNbr": 24,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2020-08-12T08:00:03Z",
      "side": 1,
      "message": "\u003e \u003e When there is no load, they will still be serialised. Where is the mechanism that makes them faster in absence of load?\n\u003e \n\u003e The sentence on lines 22-23 explains how this throttling (and parallelism) is achieved.\n\nThanks, I\u0027ll follow up in the previous comment then.",
      "parentUuid": "4102add1_3875b4b9",
      "range": {
        "startLine": 24,
        "startChar": 27,
        "endLine": 24,
        "endChar": 39
      },
      "revId": "75430697bdcfa2daa1b41c57b673d13d9e797ed4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "f99da00f_357c5619",
        "filename": "/COMMIT_MSG",
        "patchSetId": 12
      },
      "lineNbr": 24,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2020-08-12T08:00:03Z",
      "side": 1,
      "message": "\u003e \u003e I did not find any throttling mechanism in the ChainedScheduler: could you provide a source file + line number of where that is implemented? What I see from the code is a serial firing of the events using a stream.\n\u003e \n\u003e Line 104 here:\n\u003e https://gerrit-review.googlesource.com/c/plugins/replication/+/267812/12/src/main/java/com/googlesource/gerrit/plugins/replication/ChainedScheduler.java\n\u003e The next event is scheduled before executing the current event. Since an event\u0027s Task must be running before it can schedule another one, only one event can ever be scheduled concurrently.\n\nYes, that is what I understood from the code.\n\n\u003e However nothing prevents multiple event Tasks from running at the same time, and the next one is scheduled before executing the current event. If there are free threads then it is possible for more than one event to be executing at the same time.\n\nAh, gotcha. Because just before start running one you schedule the next one. And where do you throttle? How do you make sure that you\u0027re not occupying the full capacity of the execution pool? How do you give priority to the interactive events?\n\n\u003e This is what I mean by \"natural\", no special code is required to throttle this way, the \"scheduleNext then execute\" ordering takes care of the throttling.\n\nGotcha, but unless we are limiting the thread utilisation in \"some way\", I\u0027m not sure we can say we are throttling.",
      "parentUuid": "eef6cc70_f7157a9d",
      "range": {
        "startLine": 23,
        "startChar": 56,
        "endLine": 24,
        "endChar": 17
      },
      "revId": "75430697bdcfa2daa1b41c57b673d13d9e797ed4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "7e6d4627_32d92112",
        "filename": "/COMMIT_MSG",
        "patchSetId": 12
      },
      "lineNbr": 24,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2020-08-12T08:20:54Z",
      "side": 1,
      "message": "\u003e \u003e \u003e I did not find any throttling mechanism in the ChainedScheduler: could you provide a source file + line number of where that is implemented? What I see from the code is a serial firing of the events using a stream.\n\u003e \u003e \n\u003e \u003e Line 104 here:\n\u003e \u003e https://gerrit-review.googlesource.com/c/plugins/replication/+/267812/12/src/main/java/com/googlesource/gerrit/plugins/replication/ChainedScheduler.java\n\u003e \u003e The next event is scheduled before executing the current event. Since an event\u0027s Task must be running before it can schedule another one, only one event can ever be scheduled concurrently.\n\u003e \n\u003e Yes, that is what I understood from the code.\n\u003e \n\u003e \u003e However nothing prevents multiple event Tasks from running at the same time, and the next one is scheduled before executing the current event. If there are free threads then it is possible for more than one event to be executing at the same time.\n\u003e \n\u003e Ah, gotcha. Because just before start running one you schedule the next one. And where do you throttle? How do you make sure that you\u0027re not occupying the full capacity of the execution pool? How do you give priority to the interactive events?\n\nLet clarify with an example of \"throttling\", so that it becomes clearer what I was expecting to see in the code based on the commit message.\n\nIf the queue is completely empty, then execute all the replayed events as fast as you can. If there are interactive events coming through, then check that the replayed events are taking no more than X% (configurable?) of the running threads. If one replayed event, coming to a running state, goes over the X% limit, then throttle.\n\nP.S. The above is an example of throttling. Of course you can achieve that in many ways.\n\nAnother alternative solution would be, instead of throttling, just introduce the concept of \"priority queue\", exactly like the one in the airports for the security checks. The \"executor\" is shared (the number of slots for putting your stuff on the conveyor) but you could come from the general queue or from the \"priority queue\". If there is someone in the priority queue waiting, the general queue gets locked to a minimum of executors (let\u0027s say only 1 slot out of 4). If the priority queue is empty, the general queue gets the full capacity.\n\nThanks again for your patience in explaining the code.\n\n\u003e \u003e This is what I mean by \"natural\", no special code is required to throttle this way, the \"scheduleNext then execute\" ordering takes care of the throttling.\n\u003e \n\u003e Gotcha, but unless we are limiting the thread utilisation in \"some way\", I\u0027m not sure we can say we are throttling.",
      "parentUuid": "f99da00f_357c5619",
      "range": {
        "startLine": 23,
        "startChar": 56,
        "endLine": 24,
        "endChar": 17
      },
      "revId": "75430697bdcfa2daa1b41c57b673d13d9e797ed4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "13e51e5d_9f01f57b",
        "filename": "/COMMIT_MSG",
        "patchSetId": 12
      },
      "lineNbr": 24,
      "author": {
        "id": 1003873
      },
      "writtenOn": "2020-08-12T12:16:34Z",
      "side": 1,
      "message": "I also believe that I understand the ChainedScheduler, let me add a few\ncomments to the Lucas observations.\n\n\n\u003e \u003e Ah, gotcha. Because just before start running one you schedule the next one. \n\nCorrect.\n\n\u003e And where do you throttle? How do you make sure that you\u0027re not occupying the full capacity of the execution pool? How do you give priority to the interactive events?\n\nBy having at most one (pending) task in the queue at any time. As you correctly\nobserved, one task needs to be picked up from the queue (by a free thread)\nand only then the next one can go into the queue.\n\n\n\u003e Another alternative solution would be, instead of throttling, just introduce the concept of \"priority queue\", exactly like the one in the airports for the security checks. The \"executor\" is shared (the number of slots for putting your stuff on the conveyor) but you could come from the general queue or from the \"priority queue\". If there is someone in the priority queue waiting, the general queue gets locked to a minimum of executors (let\u0027s say only 1 slot out of 4). If the priority queue is empty, the general queue gets the full capacity.\n\nThis is an interesting question. \n\nLet me also try to translate the ChainedScheduler\ninto an airport based example: There is only one queue where all the passengers\nwho are *allowed* to enter the queue have the same priority.\nHowever, the passengers are classified into two groups: priority and low-prio.\nA priority passenger can enter the queue at any time.\nA low-prio passenger can only enter the queue at the moment when an already\nqueued low-prio passenger gets called to be processed.\n\nThis way, at any time, there is at most one low-prio passenger in the queue.\nIf there are no priority-passengers then the low-prio will get processed faster as\nall counters will serve them. But as soon as a priority passenger arrives\nhe will be the next or the second next, having to wait for at most one\nlow-prio passenger which is already queued.\n\n\n\u003e \n\u003e Thanks again for your patience in explaining the code.\n\u003e \n\u003e \u003e \u003e This is what I mean by \"natural\", no special code is required to throttle this way, the \"scheduleNext then execute\" ordering takes care of the throttling.\n\u003e \u003e \n\u003e \u003e Gotcha, but unless we are limiting the thread utilisation in \"some way\", I\u0027m not sure we can say we are throttling.",
      "parentUuid": "7e6d4627_32d92112",
      "range": {
        "startLine": 23,
        "startChar": 56,
        "endLine": 24,
        "endChar": 17
      },
      "revId": "75430697bdcfa2daa1b41c57b673d13d9e797ed4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "14c6cb27_5f7bb046",
        "filename": "/COMMIT_MSG",
        "patchSetId": 12
      },
      "lineNbr": 24,
      "author": {
        "id": 1003873
      },
      "writtenOn": "2020-08-12T14:23:35Z",
      "side": 1,
      "message": "Now that we have a solid understanding of the ChainedScheduler and based on\nthe analogies with the airport queues, it seems to me that an Executor\nbased on a priority queue would achieve the same effect. Martin, what do\nyou think?",
      "parentUuid": "13e51e5d_9f01f57b",
      "range": {
        "startLine": 23,
        "startChar": 56,
        "endLine": 24,
        "endChar": 17
      },
      "revId": "75430697bdcfa2daa1b41c57b673d13d9e797ed4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "8fae6871_058f9198",
        "filename": "/COMMIT_MSG",
        "patchSetId": 12
      },
      "lineNbr": 24,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2020-08-12T14:48:02Z",
      "side": 1,
      "message": "\u003e I also believe that I understand the ChainedScheduler, let me add a few\n\u003e comments to the Lucas observations.\n\u003e \n\u003e \n\u003e \u003e \u003e Ah, gotcha. Because just before start running one you schedule the next one. \n\u003e \n\u003e Correct.\n\u003e \n\u003e \u003e And where do you throttle? How do you make sure that you\u0027re not occupying the full capacity of the execution pool? How do you give priority to the interactive events?\n\u003e \n\u003e By having at most one (pending) task in the queue at any time. As you correctly\n\u003e observed, one task needs to be picked up from the queue (by a free thread)\n\u003e and only then the next one can go into the queue.\n\nHowever, during startup, Gerrit isn\u0027t active yet, it is just starting the plugins and the HTTP and SSH acceptors are not finalised.\n\nBecause the current implementation is putting another task in the queue *before* execution the fire of the replication event, they could end up occupying all replication threads at once.\n\nShould we also define an \"intial delay\" so that we don\u0027t start replaying immediately but we give time for any interactive replication event to come through?\n\n\u003e \u003e Another alternative solution would be, instead of throttling, just introduce the concept of \"priority queue\", exactly like the one in the airports for the security checks. The \"executor\" is shared (the number of slots for putting your stuff on the conveyor) but you could come from the general queue or from the \"priority queue\". If there is someone in the priority queue waiting, the general queue gets locked to a minimum of executors (let\u0027s say only 1 slot out of 4). If the priority queue is empty, the general queue gets the full capacity.\n\u003e \n\u003e This is an interesting question. \n\u003e \n\u003e Let me also try to translate the ChainedScheduler\n\u003e into an airport based example: There is only one queue where all the passengers\n\u003e who are *allowed* to enter the queue have the same priority.\n\u003e However, the passengers are classified into two groups: priority and low-prio.\n\u003e A priority passenger can enter the queue at any time.\n\u003e A low-prio passenger can only enter the queue at the moment when an already\n\u003e queued low-prio passenger gets called to be processed.\n\u003e \n\u003e This way, at any time, there is at most one low-prio passenger in the queue.\n\u003e If there are no priority-passengers then the low-prio will get processed faster as\n\u003e all counters will serve them. But as soon as a priority passenger arrives\n\u003e he will be the next or the second next, having to wait for at most one\n\u003e low-prio passenger which is already queued.\n\nThanks for the explanation, now it makes sense.\nBasically, the \"external stream of the replication events\" *IS* the low-prio passengers queue.\n\nIt would be nice to have this mechanism in the Gerrit\u0027s queue system, so that:\na. The low-prio queue backlog is visible in the gerrit show-queue command\nb. the high-prio/low-prio queueing system can be used in other parts of gerrit\n\nWith this solution, I could not see effectively how many events are waiting to be served in the low-prio queue, as the \u0027show-queue\u0027 would give me only always only 1 element for the replayed events.\n\n\u003e \u003e \n\u003e \u003e Thanks again for your patience in explaining the code.\n\u003e \u003e \n\u003e \u003e \u003e \u003e This is what I mean by \"natural\", no special code is required to throttle this way, the \"scheduleNext then execute\" ordering takes care of the throttling.\n\u003e \u003e \u003e \n\u003e \u003e \u003e Gotcha, but unless we are limiting the thread utilisation in \"some way\", I\u0027m not sure we can say we are throttling.",
      "parentUuid": "13e51e5d_9f01f57b",
      "range": {
        "startLine": 23,
        "startChar": 56,
        "endLine": 24,
        "endChar": 17
      },
      "revId": "75430697bdcfa2daa1b41c57b673d13d9e797ed4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "f3bed1d0_aef68dec",
        "filename": "/COMMIT_MSG",
        "patchSetId": 12
      },
      "lineNbr": 24,
      "author": {
        "id": 1003883
      },
      "writtenOn": "2020-08-12T14:51:48Z",
      "side": 1,
      "message": "\u003e Now that we have a solid understanding of the ChainedScheduler and based on\n\u003e the analogies with the airport queues, it seems to me that an Executor\n\u003e based on a priority queue would achieve the same effect. Martin, what do\n\u003e you think?\n\nI don\u0027t understand how one could apply a priority queue (the data structure) to this code. A PR requires some property on items in the PR that is comparable to determine the order that things come out of the queue. The order things come out of a PR is only related to that property and not to insertion order. That seems very different from what would ne desired here. What would be the objective to use a PQ?\n\nThis change implements exactly the behavior that I think is most appropriate here in a very simple way. It allows an open throttle on items when utilization is low, and it throttles things down to one item at a time (no parallelism) when utilization is high.",
      "parentUuid": "14c6cb27_5f7bb046",
      "range": {
        "startLine": 23,
        "startChar": 56,
        "endLine": 24,
        "endChar": 17
      },
      "revId": "75430697bdcfa2daa1b41c57b673d13d9e797ed4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "fbe9d46d_3407996c",
        "filename": "/COMMIT_MSG",
        "patchSetId": 12
      },
      "lineNbr": 24,
      "author": {
        "id": 1003883
      },
      "writtenOn": "2020-08-12T15:26:33Z",
      "side": 1,
      "message": "\u003e Because the current implementation is putting another task in the queue *before* execution the fire of the replication event, they could end up occupying all replication threads at once.\n\nThis is incorrect, I think there is still a miss-understanding of this change (understandable, I regularly think it is doing more than it is). This change does not change the way that startup-pending events are executed, it only changes the way they are scheduled! In other words, before this change, the replication plugin tries to schedule all the startup-pending events before letting the server startup, but it does not wait for all of those startup-pending events to execute. This change doesn\u0027t even wait for the scheduling as it only starts a ChainedScheduler in the default queue (not a replication queue). So all these startup-pending eventa will be iterated over in the default queue and each one will then be scheduled in the appropriate replication queue. There is no throttling of the startup-pending event execution, only a throttling of the scheduling to avoid overloading the default queue.\n\n\u003e Should we also define an \"intial delay\" so that we don\u0027t start replaying immediately but we give time for any interactive replication event to come through?\n\nNo since they are in another queue entirely (see above),\n\n\u003e With this solution, I could not see effectively how many events are waiting to be served in the low-prio queue, as the \u0027show-queue\u0027 would give me only always only 1 element for the replayed events.\n\nThis is true, however that is also true in the current situation. To clarify, It will always be impossible to see the unscheduled items. This change is about scheduling, not execution (there is another change about that, so this sort of discussion might apply there). So as soon as the items are scheduled, they will be scheduled/waiting and will still be visible in the replication queues once they are scheduled/waiting.",
      "parentUuid": "f3bed1d0_aef68dec",
      "range": {
        "startLine": 23,
        "startChar": 56,
        "endLine": 24,
        "endChar": 17
      },
      "revId": "75430697bdcfa2daa1b41c57b673d13d9e797ed4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "b02ade7b_3b4a97b4",
        "filename": "/COMMIT_MSG",
        "patchSetId": 12
      },
      "lineNbr": 24,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2020-08-12T20:10:27Z",
      "side": 1,
      "message": "\u003e \u003e Because the current implementation is putting another task in the queue *before* execution the fire of the replication event, they could end up occupying all replication threads at once.\n\u003e \n\u003e This is incorrect,\n\nGerrit is starting up, the acceptors (HTTP and SSH) aren\u0027t started yet, you have 100s of events to replay and 10 replication threads.\n\nThe replication plugin starts, the first event is put in the executor queue. The first thread starts execution and, as first thing, add the second event in the executor queue. The second event thread starts executing  and, as first thing, add the third event in the executor queue, (that continues until the saturation of all available threads in the replication execution thread pool).\n\nP.S. Because Gerrit is not started yet, there are no other replication events going to \"throttle\" the replayed events.\n\nCan you highlight what is incorrect in my understanding?\nFrom what I saw in the code and discussed on the change, that\u0027s my understanding on how it works at the moment.\n\n\u003e I think there is still a miss-understanding of this change (understandable, I regularly think it is doing more than it is). This change does not change the way that startup-pending events are executed, it only changes the way they are scheduled! In other words, before this change, the replication plugin tries to schedule all the startup-pending events before letting the server startup, but it does not wait for all of those startup-pending events to execute. This change doesn\u0027t even wait for the scheduling as it only starts a ChainedScheduler in the default queue (not a replication queue). So all these startup-pending eventa will be iterated over in the default queue and each one will then be scheduled in the appropriate replication queue. There is no throttling of the startup-pending event execution, only a throttling of the scheduling to avoid overloading the default queue.\n\u003e \n\u003e \u003e Should we also define an \"intial delay\" so that we don\u0027t start replaying immediately but we give time for any interactive replication event to come through?\n\u003e \n\u003e No since they are in another queue entirely (see above),\n\n*IF* (but IF) the simulation I\u0027ve described above is correct, then we do need to introduce an initial delay to prevent all the executors to get busy at startup.\n\n\u003e \u003e With this solution, I could not see effectively how many events are waiting to be served in the low-prio queue, as the \u0027show-queue\u0027 would give me only always only 1 element for the replayed events.\n\u003e \n\u003e This is true, however that is also true in the current situation.\n\nWe do not have currently the concept of \"low-prio\" events waiting to be scheduled. If we introduce a new concept (good thing, I like it) we should provide also visibility on the status of the process.\n\nA Gerrit admin would like to know \"how long would it take to consume them all?\". If we don\u0027t have visibility or metrics on the \"low-prio\" events waiting to be scheduled.\n\n\u003e To clarify, It will always be impossible to see the unscheduled items.\n\nUnless we write the code in a way that can highlight them.\n\n\u003e This change is about scheduling, not execution (there is another change about that, so this sort of discussion might apply there). So as soon as the items are scheduled, they will be scheduled/waiting and will still be visible in the replication queues once they are scheduled/waiting.\n\nTrue, but as you mentioned, the number of elements shown will always be 1, which won\u0027t answer the question \"how many are left? how long would it take to complete?\".\n\nHaving a \"low-prio\" execution backlog is a good thing, and is needed in Gerrit in general. It would be best to have it implemented once and have it well supported by existing tooling.",
      "parentUuid": "fbe9d46d_3407996c",
      "range": {
        "startLine": 23,
        "startChar": 56,
        "endLine": 24,
        "endChar": 17
      },
      "revId": "75430697bdcfa2daa1b41c57b673d13d9e797ed4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "96fa035c_5626a134",
        "filename": "/COMMIT_MSG",
        "patchSetId": 12
      },
      "lineNbr": 24,
      "author": {
        "id": 1003883
      },
      "writtenOn": "2020-08-12T21:02:01Z",
      "side": 1,
      "message": "\u003e \u003e \u003e Because the current implementation is putting another task in the queue *before* execution the fire of the replication event, they could end up occupying all replication threads at once.\n\u003e \u003e \n\u003e \u003e This is incorrect,\n\u003e \n\u003e Gerrit is starting up, the acceptors (HTTP and SSH) aren\u0027t started yet, you have 100s of events to replay and 10 replication threads.\n\u003e The replication plugin starts, the first event is put in the executor queue. The first thread starts execution and, as first thing, add the second event in the executor queue. The second event thread starts executing and, as first thing, add the third event in the executor queue,\n\nThis is correct up until this point, but only if by \"event\" you mean the scheduling of the replication event. None of this is talking about the actual pushes, only the firing. This change only alters the behavior of the firing, not the behavior of the pushes. See line 208 in the ReplicationQueue.java to understand this.\n\n\u003e (that continues until the saturation of all available threads in the replication execution thread pool).\n\nNo, the firing in this change is done in the Gerrit default queue, not in the replication queues, please see line 202 of the ReplicationQueue to understand this.\n\n...\n\u003e P.S. Because Gerrit is not started yet, there are no other replication events going to \"throttle\" the replayed events.\n\nThis change is not attempting to do this, it is only attempting to throttle the scheduling of replication events, not the pushes. This throttling occurs with respect to other things in the default queue, not to the pushes. As soon as the the first entry from the ChainedScheduler is put into the default queue the replication plugin will resume startup and the rest of the server will startup, this will not be delayed until all the pending events are scheduled.\n\n...\n\u003e True, but as you mentioned, the number of elements shown will always be 1, which won\u0027t answer the question \"how many are left? how long would it take to complete?\".\n\nAs I already have pointed out, this is an existing problem and this change is not attempting to solve that problem. This change has been under review for a long time now, and I would appreciate if you could help focus on the problem this change is trying to solve.\n\n\u003e Having a \"low-prio\" execution backlog is a good thing, and is needed in Gerrit in general. It would be best to have it implemented once and have it well supported by existing tooling.\n\nI think this is too vague of a statement to be applied and discussed on this change. Specifics matter and exact situations matter. Your review on this change has been at times very frustrating. I want your review because you often point out valuable things. I would be very grateful if you would focus more on reading the comments and the code on this change, being open minded about understanding what it is actually doing (which may require letting go of some miss-understandings), and less on other related replication issues.\n\nI think that words and explanations in commit messages are important. I used the word throttling, and I specifically described what I meant by that and when it applied. I think that I am responsible for making sure that is clear, and so far I believe it is. I believe the current miss-understanding is not related to any particular way that I worded things, but may rather be due to some preconceived notions about how throttling might be done in some contexts. My wording cannot fix that, as one has to be willing to try to understand the throttling concept I describe without assuming how it should work. I very much am starting to feel like you are not fully reading my or other people\u0027s comments (I know we likely all do that some time), could you please make a better effort at this?",
      "parentUuid": "b02ade7b_3b4a97b4",
      "range": {
        "startLine": 23,
        "startChar": 56,
        "endLine": 24,
        "endChar": 17
      },
      "revId": "75430697bdcfa2daa1b41c57b673d13d9e797ed4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "9ab2a70c_6efd55ea",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 12
      },
      "lineNbr": 0,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2020-08-11T22:29:14Z",
      "side": 1,
      "message": "I believe I start to understand more how the ChainedScheduler works. However, as it is done in a separate class for making it reusable, I would love to see some API contract validation.\n\nI am also struggling to find in the code the behaviour described in the commit message.",
      "revId": "75430697bdcfa2daa1b41c57b673d13d9e797ed4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5c790dac_90978cc0",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 12
      },
      "lineNbr": 0,
      "author": {
        "id": 1003883
      },
      "writtenOn": "2020-08-11T22:47:24Z",
      "side": 1,
      "message": "\u003e I believe I start to understand more how the ChainedScheduler works. However, as it is done in a separate class for making it reusable, I would love to see some API contract validation.\n\nWhat do you have in mind, ChainedScheduler unit tests?\n \n\u003e I am also struggling to find in the code the behaviour described in the commit message.\n\nIs this a repeat of your comment on the commit message?",
      "parentUuid": "9ab2a70c_6efd55ea",
      "revId": "75430697bdcfa2daa1b41c57b673d13d9e797ed4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "9bd00cd4_b0eaac38",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 12
      },
      "lineNbr": 0,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2020-08-12T14:48:02Z",
      "side": 1,
      "message": "Thanks Sa≈°a for your feedback.",
      "revId": "75430697bdcfa2daa1b41c57b673d13d9e797ed4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "0fcb9f4f_e53db171",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 12
      },
      "lineNbr": 0,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2020-08-12T21:16:00Z",
      "side": 1,
      "message": "I confirm my +1, I will hold off any further comments for giving space to other contributors and maintainers to dig into the change and provider their feedback.\n\nI will make further investigation on the code offline, thanks in the meantime to Martin for uploading his change and his proposed ChainScheduler. I apologise in advance if I created frustration with my questions on the code: I just wanted was deeply understand the change, its consequences on the replication plugin architecture and provide meaningful feedback.",
      "revId": "75430697bdcfa2daa1b41c57b673d13d9e797ed4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "1a146591_fb7ee8b7",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 12
      },
      "lineNbr": 0,
      "author": {
        "id": 1003883
      },
      "writtenOn": "2020-08-12T21:32:30Z",
      "side": 1,
      "message": "\u003e I just wanted was deeply understand the change, its consequences on the replication plugin architecture and provide meaningful feedback.\n\nThank you, this is very much appreciated especially as your expertise is limited!",
      "parentUuid": "0fcb9f4f_e53db171",
      "revId": "75430697bdcfa2daa1b41c57b673d13d9e797ed4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "e447db75_17763441",
        "filename": "src/main/java/com/googlesource/gerrit/plugins/replication/ChainedScheduler.java",
        "patchSetId": 12
      },
      "lineNbr": 107,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2020-08-12T08:00:03Z",
      "side": 1,
      "message": "This exception is effectively swallowed: not traced, just ignored.\nWhilst I understand the reason (thanks for the comment), it should be at least traced so that it would become easier to identify potential problems.",
      "range": {
        "startLine": 107,
        "startChar": 15,
        "endLine": 107,
        "endChar": 33
      },
      "revId": "75430697bdcfa2daa1b41c57b673d13d9e797ed4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    }
  ]
}